name: Website Optimization

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

permissions:
  contents: write

jobs:
  optimize-website:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install Dependencies
      run: |
        pip install beautifulsoup4 pillow lxml

    - name: Optimize Images and Add Lazy Loading
      run: |
        cat > optimize_images.py << 'EOF'
        #!/usr/bin/env python3
        import os
        import re
        from bs4 import BeautifulSoup
        try:
            from PIL import Image
        except ImportError:
            Image = None

        def get_image_dimensions(image_path):
            if not Image:
                return None, None
            try:
                with Image.open(image_path) as img:
                    return img.size
            except:
                return None, None

        def optimize_html_images(html_file):
            with open(html_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            soup = BeautifulSoup(content, 'html.parser')
            images = soup.find_all('img')
            changes_made = False
            
            for img in images:
                src = img.get('src', '')
                if not src or src.startswith('http' ):
                    continue
                
                # Get image dimensions
                image_path = src.lstrip('./')
                if os.path.exists(image_path):
                    width, height = get_image_dimensions(image_path)
                    if width and height:
                        if not img.get('width'):
                            img['width'] = str(width)
                            changes_made = True
                        if not img.get('height'):
                            img['height'] = str(height)
                            changes_made = True
                
                # Add loading="lazy"
                if not img.get('loading') and 'hero' not in src.lower() and 'logo' not in src.lower():
                    img['loading'] = 'lazy'
                    changes_made = True
                
                # Add object-fit style
                current_style = img.get('style', '')
                if 'object-fit' not in current_style:
                    if current_style:
                        new_style = current_style + '; object-fit: cover;'
                    else:
                        new_style = 'object-fit: cover;'
                    img['style'] = new_style
                    changes_made = True
                
                # Add alt if missing
                if not img.get('alt'):
                    if 'logo' in src.lower():
                        img['alt'] = 'Melissa Photography Paris Logo'
                    else:
                        img['alt'] = 'Professional Photography by Melissa Paris'
                    changes_made = True
            
            if changes_made:
                with open(html_file, 'w', encoding='utf-8') as f:
                    f.write(str(soup))
                print(f"Optimized: {html_file}")
            
            return changes_made

        # Process all HTML files
        html_files = []
        for root, dirs, files in os.walk('.'):
            if '.git' in root or 'node_modules' in root:
                continue
            for file in files:
                if file.endswith('.html'):
                    html_files.append(os.path.join(root, file))

        optimized_count = 0
        for html_file in html_files:
            if optimize_html_images(html_file):
                optimized_count += 1

        print(f"Optimized {optimized_count} HTML files with images")
        EOF
        
        python optimize_images.py

    - name: CSS Minification
      run: |
        cat > minify_css.py << 'EOF'
        #!/usr/bin/env python3
        import os
        import re
        from bs4 import BeautifulSoup

        def simple_css_minify(css_content):
            # Remove comments
            css_content = re.sub(r'/\*.*?\*/', '', css_content, flags=re.DOTALL)
            # Remove extra whitespace
            css_content = re.sub(r'\s+', ' ', css_content)
            # Remove spaces around certain characters
            css_content = re.sub(r'\s*([{}:;,>+~])\s*', r'\1', css_content)
            return css_content.strip()

        def minify_css_file(css_file):
            with open(css_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            minified = simple_css_minify(content)
            
            # Create minified version
            base_name = os.path.splitext(css_file)[0]
            min_file = f"{base_name}.min.css"
            
            with open(min_file, 'w', encoding='utf-8') as f:
                f.write(minified)
            
            return min_file

        def update_html_css_refs(html_file):
            with open(html_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            soup = BeautifulSoup(content, 'html.parser')
            links = soup.find_all('link', rel='stylesheet')
            changes_made = False
            
            for link in links:
                href = link.get('href', '')
                if href and not href.startswith('http' ) and href.endswith('.css') and not href.endswith('.min.css'):
                    min_href = href.replace('.css', '.min.css')
                    if os.path.exists(min_href.lstrip('./')):
                        link['href'] = min_href
                        changes_made = True
            
            if changes_made:
                with open(html_file, 'w', encoding='utf-8') as f:
                    f.write(str(soup))
            
            return changes_made

        # Find and minify CSS files
        css_files = []
        for root, dirs, files in os.walk('.'):
            if '.git' in root or 'node_modules' in root:
                continue
            for file in files:
                if file.endswith('.css') and not file.endswith('.min.css'):
                    css_files.append(os.path.join(root, file))

        minified_count = 0
        for css_file in css_files:
            try:
                min_file = minify_css_file(css_file)
                minified_count += 1
                print(f"Minified: {css_file}")
            except Exception as e:
                print(f"Error minifying {css_file}: {e}")

        # Update HTML references
        html_files = []
        for root, dirs, files in os.walk('.'):
            if '.git' in root:
                continue
            for file in files:
                if file.endswith('.html'):
                    html_files.append(os.path.join(root, file))

        updated_count = 0
        for html_file in html_files:
            if update_html_css_refs(html_file):
                updated_count += 1

        print(f"Minified {minified_count} CSS files, updated {updated_count} HTML files")
        EOF
        
        python minify_css.py

    - name: SEO Optimization
      run: |
        cat > optimize_seo.py << 'EOF'
        #!/usr/bin/env python3
        import os
        from bs4 import BeautifulSoup

        def optimize_seo(html_file):
            with open(html_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            soup = BeautifulSoup(content, 'html.parser')
            changes_made = False
            
            # Add lang attribute to html tag
            html_tag = soup.find('html')
            if html_tag and not html_tag.get('lang'):
                if 'fr' in os.path.basename(html_file):
                    html_tag['lang'] = 'fr'
                else:
                    html_tag['lang'] = 'en'
                changes_made = True
            
            # Add meta viewport if missing
            if not soup.find('meta', attrs={'name': 'viewport'}):
                if soup.head:
                    meta_viewport = soup.new_tag('meta')
                    meta_viewport.attrs['name'] = 'viewport'
                    meta_viewport.attrs['content'] = 'width=device-width, initial-scale=1.0'
                    soup.head.append(meta_viewport)
                    changes_made = True
            
            # Add meta description if missing
            if not soup.find('meta', attrs={'name': 'description'}):
                if soup.head:
                    meta_desc = soup.new_tag('meta')
                    meta_desc.attrs['name'] = 'description'
                    meta_desc.attrs['content'] = 'Professional photographer in Paris - Melissa Photography. Specializing in proposals, couples, weddings and portraits.'
                    soup.head.append(meta_desc)
                    changes_made = True
            
            if changes_made:
                with open(html_file, 'w', encoding='utf-8') as f:
                    f.write(str(soup))
                print(f"SEO optimized: {html_file}")
            
            return changes_made

        # Process all HTML files
        html_files = []
        for root, dirs, files in os.walk('.'):
            if '.git' in root:
                continue
            for file in files:
                if file.endswith('.html'):
                    html_files.append(os.path.join(root, file))

        optimized_count = 0
        for html_file in html_files:
            if optimize_seo(html_file):
                optimized_count += 1

        print(f"SEO optimized {optimized_count} HTML files")
        EOF
        
        python optimize_seo.py

    - name: Update Sitemap and Robots
      run: |
        cat > update_sitemap.py << 'EOF'
        #!/usr/bin/env python3
        import os
        from datetime import datetime

        # Generate sitemap.xml
        sitemap_content = '''<?xml version="1.0" encoding="UTF-8"?>
        <urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
        '''

        base_url = 'https://melissaphotography.paris'
        current_date = datetime.now( ).strftime('%Y-%m-%d')

        # Find all HTML files
        html_files = []
        for root, dirs, files in os.walk('.'):
            if '.git' in root or 'fonts' in root:
                continue
            for file in files:
                if file.endswith('.html') and not file.startswith('demo'):
                    rel_path = os.path.relpath(os.path.join(root, file), '.')
                    if rel_path.count('/') <= 1:
                        html_files.append(rel_path)

        # Page priorities
        priorities = {
            'index.html': '1.0',
            'indexfr.html': '1.0',
            'services.html': '0.9',
            'servicesfr.html': '0.9',
            'about.html': '0.8',
            'aboutfr.html': '0.8'
        }

        for page in sorted(html_files):
            if page == 'index.html':
                url = f'{base_url}/'
            else:
                url = f'{base_url}/{page}'
            
            priority = priorities.get(page, '0.5')
            
            sitemap_content += f'''  <url>
            <loc>{url}</loc>
            <lastmod>{current_date}</lastmod>
            <changefreq>monthly</changefreq>
            <priority>{priority}</priority>
          </url>
        '''

        sitemap_content += '</urlset>'

        with open('sitemap.xml', 'w', encoding='utf-8') as f:
            f.write(sitemap_content)

        # Update robots.txt
        robots_content = f'''User-agent: *
        Allow: /

        Sitemap: {base_url}/sitemap.xml

        Disallow: /admin/
        Disallow: /.git/
        Disallow: /.vscode/
        Disallow: /scss/

        Allow: /css/
        Allow: /js/
        Allow: /images/
        Allow: /fonts/
        '''

        with open('robots.txt', 'w') as f:
            f.write(robots_content)

        print(f"Updated sitemap.xml with {len(html_files)} pages")
        print("Updated robots.txt")
        EOF
        
        python update_sitemap.py

    - name: Create Security Headers
      run: |
        cat > _headers << 'EOF'
        /*
          X-Frame-Options: DENY
          X-Content-Type-Options: nosniff
          X-XSS-Protection: 1; mode=block
          Referrer-Policy: strict-origin-when-cross-origin
          Strict-Transport-Security: max-age=31536000; includeSubDomains

        /css/*
          Cache-Control: public, max-age=31536000

        /js/*
          Cache-Control: public, max-age=31536000

        /images/*
          Cache-Control: public, max-age=2592000
        EOF
        echo "Updated security headers"

    - name: Commit Changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "Website Optimizer"
        
        if git diff --quiet && git diff --staged --quiet; then
          echo "No changes to commit"
        else
          git add .
          git commit -m "Auto-optimization: Images, CSS, SEO, Security improvements" || echo "Nothing to commit"
          git push || echo "Nothing to push"
        fi

    - name: Summary
      run: |
        echo "Website optimization completed successfully!"
        echo "Optimizations applied:"
        echo "- Image lazy loading and dimensions"
        echo "- CSS minification"
        echo "- SEO meta tags"
        echo "- Updated sitemap and robots.txt"
        echo "- Security headers"
