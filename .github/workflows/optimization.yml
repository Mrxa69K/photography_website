name: Website Optimization

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

permissions:
  contents: write

jobs:
  optimize-website:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Check Repository Structure
      run: |
        echo "Repository structure:"
        ls -la
        echo ""
        echo "Looking for website files..."
        find . -name "*.html" -type f | head -10
        echo ""
        echo "Looking for directories..."
        find . -type d -name "*version*" -o -name "*website*" -o -name "*final*" | head -10

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install Dependencies
      run: |
        pip install beautifulsoup4 pillow lxml

    - name: Find Website Directory and Optimize
      run: |
        # Find the correct website directory
        WEBSITE_DIR="."
        
        if [ -d "final-version" ]; then
          WEBSITE_DIR="final-version"
          echo "Found website files in: final-version/"
        elif [ -d "WEBSITE/final-version" ]; then
          WEBSITE_DIR="WEBSITE/final-version"
          echo "Found website files in: WEBSITE/final-version/"
        elif [ -d "website" ]; then
          WEBSITE_DIR="website"
          echo "Found website files in: website/"
        else
          echo "Using root directory"
        fi
        
        echo "Working in directory: $WEBSITE_DIR"
        cd "$WEBSITE_DIR"
        
        # Create optimization script
        cat > optimize_images.py << 'EOF'
        #!/usr/bin/env python3
        import os
        import re
        from bs4 import BeautifulSoup
        try:
            from PIL import Image
        except ImportError:
            Image = None

        def get_image_dimensions(image_path):
            if not Image:
                return None, None
            try:
                with Image.open(image_path) as img:
                    return img.size
            except:
                return None, None

        def optimize_html_images(html_file):
            print(f"Processing: {html_file}")
            with open(html_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            soup = BeautifulSoup(content, 'html.parser')
            images = soup.find_all('img')
            changes_made = False
            
            for img in images:
                src = img.get('src', '')
                if not src or src.startswith('http' ):
                    continue
                
                # Get image dimensions
                image_path = src.lstrip('./')
                if os.path.exists(image_path):
                    width, height = get_image_dimensions(image_path)
                    if width and height:
                        if not img.get('width'):
                            img['width'] = str(width)
                            changes_made = True
                        if not img.get('height'):
                            img['height'] = str(height)
                            changes_made = True
                
                # Add loading="lazy"
                if not img.get('loading') and 'hero' not in src.lower() and 'logo' not in src.lower():
                    img['loading'] = 'lazy'
                    changes_made = True
                
                # Add object-fit style
                current_style = img.get('style', '')
                if 'object-fit' not in current_style:
                    if current_style:
                        new_style = current_style + '; object-fit: cover;'
                    else:
                        new_style = 'object-fit: cover;'
                    img['style'] = new_style
                    changes_made = True
                
                # Add alt if missing
                if not img.get('alt'):
                    if 'logo' in src.lower():
                        img['alt'] = 'Melissa Photography Paris Logo'
                    else:
                        img['alt'] = 'Professional Photography by Melissa Paris'
                    changes_made = True
            
            if changes_made:
                with open(html_file, 'w', encoding='utf-8') as f:
                    f.write(str(soup))
                print(f"✅ Optimized: {html_file}")
                return True
            else:
                print(f"⏭️ No changes needed: {html_file}")
                return False

        # Process all HTML files
        html_files = []
        for root, dirs, files in os.walk('.'):
            if '.git' in root or 'node_modules' in root:
                continue
            for file in files:
                if file.endswith('.html'):
                    html_files.append(os.path.join(root, file))

        print(f"Found {len(html_files)} HTML files")
        optimized_count = 0
        for html_file in html_files:
            if optimize_html_images(html_file):
                optimized_count += 1

        print(f"🎉 Optimized {optimized_count} HTML files with images")
        EOF
        
        python optimize_images.py
        
        # CSS Minification
        cat > minify_css.py << 'EOF'
        #!/usr/bin/env python3
        import os
        import re
        from bs4 import BeautifulSoup

        def simple_css_minify(css_content):
            # Remove comments
            css_content = re.sub(r'/\*.*?\*/', '', css_content, flags=re.DOTALL)
            # Remove extra whitespace
            css_content = re.sub(r'\s+', ' ', css_content)
            # Remove spaces around certain characters
            css_content = re.sub(r'\s*([{}:;,>+~])\s*', r'\1', css_content)
            return css_content.strip()

        def minify_css_file(css_file):
            with open(css_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            minified = simple_css_minify(content)
            
            # Create minified version
            base_name = os.path.splitext(css_file)[0]
            min_file = f"{base_name}.min.css"
            
            with open(min_file, 'w', encoding='utf-8') as f:
                f.write(minified)
            
            return min_file

        # Find and minify CSS files
        css_files = []
        for root, dirs, files in os.walk('.'):
            if '.git' in root or 'node_modules' in root:
                continue
            for file in files:
                if file.endswith('.css') and not file.endswith('.min.css'):
                    css_files.append(os.path.join(root, file))

        print(f"Found {len(css_files)} CSS files to minify")
        minified_count = 0
        for css_file in css_files:
            try:
                min_file = minify_css_file(css_file)
                minified_count += 1
                print(f"✅ Minified: {css_file}")
            except Exception as e:
                print(f"❌ Error minifying {css_file}: {e}")

        print(f"🎉 Minified {minified_count} CSS files")
        EOF
        
        python minify_css.py
        
        # SEO Optimization
        cat > optimize_seo.py << 'EOF'
        #!/usr/bin/env python3
        import os
        from bs4 import BeautifulSoup

        def optimize_seo(html_file):
            with open(html_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            soup = BeautifulSoup(content, 'html.parser')
            changes_made = False
            
            # Add lang attribute to html tag
            html_tag = soup.find('html')
            if html_tag and not html_tag.get('lang'):
                if 'fr' in os.path.basename(html_file):
                    html_tag['lang'] = 'fr'
                else:
                    html_tag['lang'] = 'en'
                changes_made = True
            
            # Add meta viewport if missing
            if not soup.find('meta', attrs={'name': 'viewport'}):
                if soup.head:
                    meta_viewport = soup.new_tag('meta')
                    meta_viewport.attrs['name'] = 'viewport'
                    meta_viewport.attrs['content'] = 'width=device-width, initial-scale=1.0'
                    soup.head.append(meta_viewport)
                    changes_made = True
            
            # Add meta description if missing
            if not soup.find('meta', attrs={'name': 'description'}):
                if soup.head:
                    meta_desc = soup.new_tag('meta')
                    meta_desc.attrs['name'] = 'description'
                    meta_desc.attrs['content'] = 'Professional photographer in Paris - Melissa Photography. Specializing in proposals, couples, weddings and portraits.'
                    soup.head.append(meta_desc)
                    changes_made = True
            
            if changes_made:
                with open(html_file, 'w', encoding='utf-8') as f:
                    f.write(str(soup))
                print(f"✅ SEO optimized: {html_file}")
                return True
            return False

        # Process all HTML files
        html_files = []
        for root, dirs, files in os.walk('.'):
            if '.git' in root:
                continue
            for file in files:
                if file.endswith('.html'):
                    html_files.append(os.path.join(root, file))

        optimized_count = 0
        for html_file in html_files:
            if optimize_seo(html_file):
                optimized_count += 1

        print(f"🎉 SEO optimized {optimized_count} HTML files")
        EOF
        
        python optimize_seo.py
        
        # Update sitemap and robots
        cat > update_config.py << 'EOF'
        #!/usr/bin/env python3
        import os
        from datetime import datetime

        # Generate sitemap.xml
        sitemap_content = '''<?xml version="1.0" encoding="UTF-8"?>
        <urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
        '''

        base_url = 'https://melissaphotography.paris'
        current_date = datetime.now( ).strftime('%Y-%m-%d')

        # Find all HTML files
        html_files = []
        for root, dirs, files in os.walk('.'):
            if '.git' in root or 'fonts' in root:
                continue
            for file in files:
                if file.endswith('.html') and not file.startswith('demo'):
                    rel_path = os.path.relpath(os.path.join(root, file), '.')
                    if rel_path.count('/') <= 1:
                        html_files.append(rel_path)

        for page in sorted(html_files):
            if page == 'index.html':
                url = f'{base_url}/'
            else:
                url = f'{base_url}/{page}'
            
            sitemap_content += f'''  <url>
            <loc>{url}</loc>
            <lastmod>{current_date}</lastmod>
            <changefreq>monthly</changefreq>
            <priority>0.8</priority>
          </url>
        '''

        sitemap_content += '</urlset>'

        with open('sitemap.xml', 'w', encoding='utf-8') as f:
            f.write(sitemap_content)

        # Update robots.txt
        robots_content = f'''User-agent: *
        Allow: /
        Sitemap: {base_url}/sitemap.xml
        '''

        with open('robots.txt', 'w') as f:
            f.write(robots_content)

        print(f"✅ Updated sitemap.xml with {len(html_files)} pages")
        print("✅ Updated robots.txt")
        EOF
        
        python update_config.py
        
        # Create security headers
        cat > _headers << 'EOF'
        /*
          X-Frame-Options: DENY
          X-Content-Type-Options: nosniff
          X-XSS-Protection: 1; mode=block
          Referrer-Policy: strict-origin-when-cross-origin
          Strict-Transport-Security: max-age=31536000; includeSubDomains

        /css/*
          Cache-Control: public, max-age=31536000

        /js/*
          Cache-Control: public, max-age=31536000

        /images/*
          Cache-Control: public, max-age=2592000
        EOF
        
        echo "✅ Created security headers"
        
        # Go back to root for git operations
        cd ..

    - name: Commit Changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "Website Optimizer"
        
        if git diff --quiet && git diff --staged --quiet; then
          echo "No changes to commit"
        else
          git add .
          git commit -m "🚀 Auto-optimization: Images, CSS, SEO, Security improvements" || echo "Nothing to commit"
          git push || echo "Nothing to push"
        fi

    - name: Summary
      run: |
        echo "🎉 Website optimization completed successfully!"
        echo "✅ Image lazy loading and dimensions added"
        echo "✅ CSS files minified"
        echo "✅ SEO meta tags enhanced"
        echo "✅ Sitemap and robots.txt updated"
        echo "✅ Security headers created"
