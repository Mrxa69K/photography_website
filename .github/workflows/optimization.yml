name: 🚀 Website Optimization & Auto-Deploy

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch: # Manual trigger

permissions:
  contents: write

jobs:
  optimize-website:
    runs-on: ubuntu-latest
    
    steps:
    - name: 📥 Checkout Repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: 🐍 Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: 📦 Install Dependencies
      run: |
        pip install beautifulsoup4 pillow lxml cssmin htmlmin

    - name: 🖼️ Optimize Images & Add Lazy Loading
      run: |
        cat > optimize_images.py << 'EOF'
        #!/usr/bin/env python3
        import os
        import re
        from PIL import Image
        from bs4 import BeautifulSoup

        def get_image_dimensions(image_path):
            try:
                with Image.open(image_path) as img:
                    return img.size
            except:
                return None, None

        def optimize_html_images(html_file):
            with open(html_file, 'r', encoding='utf-8') as f:
                soup = BeautifulSoup(f.read(), 'html.parser')
            
            images = soup.find_all('img')
            changes_made = False
            
            for img in images:
                src = img.get('src', '')
                if not src or src.startswith('http' ):
                    continue
                
                # Get image dimensions
                image_path = src.lstrip('./')
                if os.path.exists(image_path):
                    width, height = get_image_dimensions(image_path)
                    if width and height:
                        if not img.get('width'):
                            img['width'] = str(width)
                            changes_made = True
                        if not img.get('height'):
                            img['height'] = str(height)
                            changes_made = True
                
                # Add loading="lazy" (except for hero images)
                if not img.get('loading') and 'hero' not in src.lower() and 'logo' not in src.lower():
                    img['loading'] = 'lazy'
                    changes_made = True
                
                # Add object-fit style
                current_style = img.get('style', '')
                if 'object-fit' not in current_style:
                    new_style = current_style + '; object-fit: cover;' if current_style else 'object-fit: cover;'
                    img['style'] = new_style
                    changes_made = True
                
                # Add alt if missing
                if not img.get('alt'):
                    if 'logo' in src.lower():
                        img['alt'] = 'Melissa Photography Paris Logo'
                    else:
                        img['alt'] = 'Professional Photography by Melissa Paris'
                    changes_made = True
            
            if changes_made:
                with open(html_file, 'w', encoding='utf-8') as f:
                    f.write(str(soup))
                print(f"✅ Optimized: {html_file}")
            
            return changes_made

        # Process all HTML files
        html_files = []
        for root, dirs, files in os.walk('.'):
            if '.git' in root or 'node_modules' in root:
                continue
            for file in files:
                if file.endswith('.html'):
                    html_files.append(os.path.join(root, file))

        optimized_count = 0
        for html_file in html_files:
            if optimize_html_images(html_file):
                optimized_count += 1

        print(f"🎉 Optimized {optimized_count} HTML files with images")
        EOF
        
        python optimize_images.py

    - name: 🎨 CSS Optimization & Minification
      run: |
        cat > optimize_css.py << 'EOF'
        #!/usr/bin/env python3
        import os
        import re
        from cssmin import cssmin
        from bs4 import BeautifulSoup

        def minify_css_file(css_file):
            with open(css_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            minified = cssmin(content)
            
            # Create minified version
            base_name = os.path.splitext(css_file)[0]
            min_file = f"{base_name}.min.css"
            
            with open(min_file, 'w', encoding='utf-8') as f:
                f.write(minified)
            
            return min_file

        def update_html_css_refs(html_file):
            with open(html_file, 'r', encoding='utf-8') as f:
                soup = BeautifulSoup(f.read(), 'html.parser')
            
            links = soup.find_all('link', rel='stylesheet')
            changes_made = False
            
            for link in links:
                href = link.get('href', '')
                if href and not href.startswith('http' ) and href.endswith('.css') and not href.endswith('.min.css'):
                    min_href = href.replace('.css', '.min.css')
                    if os.path.exists(min_href.lstrip('./')):
                        link['href'] = min_href
                        changes_made = True
            
            if changes_made:
                with open(html_file, 'w', encoding='utf-8') as f:
                    f.write(str(soup))
            
            return changes_made

        # Find and minify CSS files
        css_files = []
        for root, dirs, files in os.walk('.'):
            if '.git' in root or 'node_modules' in root:
                continue
            for file in files:
                if file.endswith('.css') and not file.endswith('.min.css'):
                    css_files.append(os.path.join(root, file))

        minified_count = 0
        for css_file in css_files:
            try:
                min_file = minify_css_file(css_file)
                minified_count += 1
                print(f"✅ Minified: {css_file} → {min_file}")
            except Exception as e:
                print(f"❌ Error minifying {css_file}: {e}")

        # Update HTML references
        html_files = []
        for root, dirs, files in os.walk('.'):
            if '.git' in root:
                continue
            for file in files:
                if file.endswith('.html'):
                    html_files.append(os.path.join(root, file))

        updated_count = 0
        for html_file in html_files:
            if update_html_css_refs(html_file):
                updated_count += 1

        print(f"🎉 Minified {minified_count} CSS files, updated {updated_count} HTML files")
        EOF
        
        python optimize_css.py

    - name: 🔍 SEO & Meta Tags Optimization
      run: |
        cat > optimize_seo.py << 'EOF'
        #!/usr/bin/env python3
        import os
        from bs4 import BeautifulSoup

        def optimize_seo(html_file):
            with open(html_file, 'r', encoding='utf-8') as f:
                soup = BeautifulSoup(f.read(), 'html.parser')
            
            changes_made = False
            
            # Add lang attribute to html tag
            html_tag = soup.find('html')
            if html_tag and not html_tag.get('lang'):
                if 'fr' in os.path.basename(html_file):
                    html_tag['lang'] = 'fr'
                else:
                    html_tag['lang'] = 'en'
                changes_made = True
            
            # Add meta viewport if missing
            if not soup.find('meta', attrs={'name': 'viewport'}):
                if soup.head:
                    meta_viewport = soup.new_tag('meta', attrs={
                        'name': 'viewport', 
                        'content': 'width=device-width, initial-scale=1.0'
                    })
                    soup.head.append(meta_viewport)
                    changes_made = True
            
            # Add meta description if missing
            if not soup.find('meta', attrs={'name': 'description'}):
                if soup.head:
                    meta_desc = soup.new_tag('meta', attrs={
                        'name': 'description', 
                        'content': 'Professional photographer in Paris - Melissa Photography. Specializing in proposals, couples, weddings and portraits.'
                    })
                    soup.head.append(meta_desc)
                    changes_made = True
            
            # Add meta robots if missing
            if not soup.find('meta', attrs={'name': 'robots'}):
                if soup.head:
                    meta_robots = soup.new_tag('meta', attrs={
                        'name': 'robots', 
                        'content': 'index, follow'
                    })
                    soup.head.append(meta_robots)
                    changes_made = True
            
            if changes_made:
                with open(html_file, 'w', encoding='utf-8') as f:
                    f.write(str(soup))
                print(f"✅ SEO optimized: {html_file}")
            
            return changes_made

        # Process all HTML files
        html_files = []
        for root, dirs, files in os.walk('.'):
            if '.git' in root:
                continue
            for file in files:
                if file.endswith('.html'):
                    html_files.append(os.path.join(root, file))

        optimized_count = 0
        for html_file in html_files:
            if optimize_seo(html_file):
                optimized_count += 1

        print(f"🎉 SEO optimized {optimized_count} HTML files")
        EOF
        
        python optimize_seo.py

    - name: 🗺️ Update Sitemap & Robots.txt
      run: |
        cat > update_sitemap.py << 'EOF'
        #!/usr/bin/env python3
        import os
        import xml.etree.ElementTree as ET
        from datetime import datetime

        # Generate sitemap.xml
        urlset = ET.Element('urlset')
        urlset.set('xmlns', 'http://www.sitemaps.org/schemas/sitemap/0.9' )

        base_url = 'https://melissaphotography.paris'
        current_date = datetime.now( ).strftime('%Y-%m-%d')

        # Find all HTML files
        html_files = []
        for root, dirs, files in os.walk('.'):
            if '.git' in root or 'fonts' in root:
                continue
            for file in files:
                if file.endswith('.html') and not file.startswith('demo'):
                    rel_path = os.path.relpath(os.path.join(root, file), '.')
                    if rel_path.count('/') <= 1:  # Main pages only
                        html_files.append(rel_path)

        # Page priorities
        priorities = {
            'index.html': '1.0',
            'indexfr.html': '1.0',
            'services.html': '0.9',
            'servicesfr.html': '0.9',
            'about.html': '0.8',
            'aboutfr.html': '0.8',
            'proposal.html': '0.8',
            'proposalfr.html': '0.8',
            'wedding.html': '0.8',
            'weddingfr.html': '0.8',
            'contact.html': '0.7',
            'contactfr.html': '0.7',
        }

        for page in sorted(html_files):
            url = ET.SubElement(urlset, 'url')
            
            loc = ET.SubElement(url, 'loc')
            if page == 'index.html':
                loc.text = f'{base_url}/'
            else:
                loc.text = f'{base_url}/{page}'
            
            lastmod = ET.SubElement(url, 'lastmod')
            lastmod.text = current_date
            
            changefreq = ET.SubElement(url, 'changefreq')
            changefreq.text = 'monthly'
            
            priority = ET.SubElement(url, 'priority')
            priority.text = priorities.get(page, '0.5')

        # Save sitemap
        tree = ET.ElementTree(urlset)
        ET.indent(tree, space="  ", level=0)
        tree.write('sitemap.xml', encoding='utf-8', xml_declaration=True)

        # Update robots.txt
        robots_content = f"""User-agent: *
Allow: /

# Sitemap
Sitemap: {base_url}/sitemap.xml

# Disallow admin and development areas
Disallow: /admin/
Disallow: /.git/
Disallow: /.vscode/
Disallow: /scss/
Disallow: /node_modules/

# Allow important pages
Allow: /css/
Allow: /js/
Allow: /images/
Allow: /fonts/

# Crawl delay
Crawl-delay: 1
"""

        with open('robots.txt', 'w') as f:
            f.write(robots_content)

        print(f"✅ Updated sitemap.xml with {len(html_files)} pages")
        print("✅ Updated robots.txt")
        EOF
        
        python update_sitemap.py

    - name: 🔐 Security Headers Optimization
      run: |
        cat > _headers << 'EOF'
/*
  # Security Headers
  X-Frame-Options: DENY
  X-Content-Type-Options: nosniff
  X-XSS-Protection: 1; mode=block
  Referrer-Policy: strict-origin-when-cross-origin
  Permissions-Policy: camera=(), microphone=(), geolocation=()
  Strict-Transport-Security: max-age=31536000; includeSubDomains; preload
  
  # Content Security Policy
  Content-Security-Policy: default-src 'self'; script-src 'self' 'unsafe-inline' cdnjs.cloudflare.com cdn.jsdelivr.net; style-src 'self' 'unsafe-inline' fonts.googleapis.com cdnjs.cloudflare.com cdn.jsdelivr.net; font-src 'self' fonts.gstatic.com; img-src 'self' data: https:; connect-src 'self'; frame-ancestors 'none';

# Cache Control for performance
/css/*
  Cache-Control: public, max-age=31536000, immutable

/js/*
  Cache-Control: public, max-age=31536000, immutable

/images/*
  Cache-Control: public, max-age=2592000

/fonts/*
  Cache-Control: public, max-age=31536000, immutable

/*.html
  Cache-Control: public, max-age=3600

/
  Cache-Control: public, max-age=3600
EOF
        echo "✅ Updated security headers"

    - name: 📊 Generate Optimization Report
      run: |
        echo "# 🚀 Website Optimization Report" > optimization-report.md
        echo "" >> optimization-report.md
        echo "**Date:** $(date )" >> optimization-report.md
        echo "**Commit:** ${{ github.sha }}" >> optimization-report.md
        echo "" >> optimization-report.md
        echo "## ✅ Optimizations Applied" >> optimization-report.md
        echo "" >> optimization-report.md
        echo "- 🖼️ **Images**: Added width/height, loading='lazy', object-fit, alt attributes" >> optimization-report.md
        echo "- 🎨 **CSS**: Minified all CSS files for better performance" >> optimization-report.md
        echo "- 🔍 **SEO**: Enhanced meta tags, lang attributes, viewport settings" >> optimization-report.md
        echo "- 🗺️ **Sitemap**: Updated with all pages for better crawling" >> optimization-report.md
        echo "- 🤖 **Robots.txt**: Optimized for search engine crawling" >> optimization-report.md
        echo "- 🔐 **Security**: Enhanced headers and CSP for better security" >> optimization-report.md
        echo "" >> optimization-report.md
        echo "## 📈 Expected Improvements" >> optimization-report.md
        echo "" >> optimization-report.md
        echo "- **Performance Score**: 85-95+ (Lighthouse)" >> optimization-report.md
        echo "- **SEO Score**: 95-100" >> optimization-report.md
        echo "- **Core Web Vitals**: Significantly improved" >> optimization-report.md
        echo "- **Security**: Enhanced with proper headers" >> optimization-report.md
        
        echo "✅ Generated optimization report"

    - name: 🔍 Validate HTML Files
      run: |
        echo "🔍 Validating HTML structure..."
        python -c "
import os
from bs4 import BeautifulSoup

errors = 0
for root, dirs, files in os.walk('.'):
    if '.git' in root:
        continue
    for file in files:
        if file.endswith('.html'):
            try:
                with open(os.path.join(root, file), 'r', encoding='utf-8') as f:
                    soup = BeautifulSoup(f.read(), 'html.parser')
                print(f'✅ Valid: {file}')
            except Exception as e:
                print(f'❌ Error in {file}: {e}')
                errors += 1

if errors == 0:
    print('🎉 All HTML files are valid!')
else:
    print(f'⚠️ Found {errors} HTML validation errors')
"

    - name: 📝 Commit Optimizations
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "Website Optimizer Bot"
        
        # Check if there are changes
        if git diff --quiet && git diff --staged --quiet; then
          echo "No changes to commit"
        else
          git add .
          git commit -m "🚀 Auto-optimization: Images, CSS, SEO, Security & Performance

- Added loading='lazy' to images for better performance
- Applied object-fit: cover for proper aspect ratios  
- Completed alt attributes for accessibility
- Minified CSS files for faster loading
- Enhanced SEO with meta tags and structure
- Updated sitemap.xml and robots.txt
- Improved security headers
- Generated optimization report

[skip ci]" || echo "Nothing to commit"
          
          # Push changes back to repository
          git push || echo "Nothing to push"
        fi

    - name: 🎉 Optimization Complete
      run: |
        echo "🎉 Website optimization completed successfully!"
        echo "📊 Check the optimization-report.md for details"
        echo "🚀 Your website is now optimized for:"
        echo "   - Better mobile performance"
        echo "   - Improved SEO rankings"
        echo "   - Enhanced security"
        echo "   - Faster loading times"
        echo "   - Better Core Web Vitals"
